{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def upload_file(files):\n",
    "    file_paths = [file.name for file in files]\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "def check_if_folder_has_images(file_paths):\n",
    "    return len(file_paths) > 0\n",
    "\n",
    "\n",
    "def show_state_of_files(file_paths):\n",
    "    print(file_paths.value)\n",
    "\n",
    "\n",
    "def rank(images, prompt):\n",
    "    # randomize order of images\n",
    "    images = [file.name for file in images]\n",
    "    return np.random.permutation(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (!f()).\n",
      "Your token has been saved to /root/.huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\", add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from typing import Union\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from capit.models import *\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from capit.core.models import (\n",
    "    CLIPImageTextModel,\n",
    "    CLIPWithPostProcessingImageTextModel,\n",
    "    CAPCLIPImageTextModel,\n",
    ")\n",
    "from capit.core.data.datasets import ImageTextRetrievalInput\n",
    "\n",
    "\n",
    "class Ranker(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: Union[\n",
    "            CLIPImageTextModel,\n",
    "            CLIPWithPostProcessingImageTextModel,\n",
    "            CAPCLIPImageTextModel,\n",
    "        ],\n",
    "        model_name_or_path: str,\n",
    "        repo_path: str,\n",
    "        model_name: str,\n",
    "        batch: ImageTextRetrievalInput,\n",
    "        cache_path: str = \".cache/\",\n",
    "        pretrained: bool = True,\n",
    "        backbone_fine_tunable: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if model_type != CAPCLIPImageTextModel:\n",
    "            self.model = model_type(\n",
    "                pretrained=pretrained, model_name_or_path=model_name_or_path\n",
    "            )\n",
    "        else:\n",
    "            self.model = model_type(\n",
    "                pretrained=pretrained,\n",
    "                model_name_or_path=model_name_or_path,\n",
    "                backbone_fine_tunable=backbone_fine_tunable,\n",
    "            )\n",
    "        self.model.build(batch=batch)\n",
    "        self.model = self.model.to(torch.cuda.current_device())\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.pretrained = pretrained\n",
    "        self.model.load_from_repo(\n",
    "            repo_path=repo_path, model_name=model_name, cache_path=cache_path\n",
    "        )\n",
    "\n",
    "    def rank(\n",
    "        self, prompt_text, challenge_image_paths, collection_image_paths=None\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            challenge_images = [\n",
    "                transforms.ToTensor()(Image.open(file.name))\n",
    "                for file in challenge_image_paths\n",
    "            ]\n",
    "            if collection_image_paths is not None:\n",
    "                collection_images = [\n",
    "                    transforms.ToTensor()(Image.open(file.name))\n",
    "                    for file in collection_image_paths\n",
    "                ]\n",
    "            else:\n",
    "                collection_images = None\n",
    "\n",
    "            similarities = self.model.forward(\n",
    "                challenge_images=challenge_images,\n",
    "                collection_images=collection_images,\n",
    "                prompt_text=prompt_text,\n",
    "            )\n",
    "            rank_similarities_args = torch.argsort(\n",
    "                similarities.logits_per_image, descending=True\n",
    "            )[0]\n",
    "            return [\n",
    "                challenge_image_paths[i].name for i in rank_similarities_args\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hasattr(CLIPImageTextModel, \"load_from_repo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs = ImageTextRetrievalInput(\n",
    "    target_image=torch.rand(1, 3, 224, 224),\n",
    "    challenge_images=torch.rand(1, 50, 3, 224, 224),\n",
    "    challenge_paths=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"],\n",
    "    target_text=[\"a picture of a cat\"],\n",
    "    collection_images=torch.rand(1, 15, 3, 224, 224),\n",
    "    collection_paths=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Ranker(\n",
    "    model_type=CLIPImageTextModel,\n",
    "    model_name_or_path=\"openai/clip-vit-large-patch14\",\n",
    "    repo_path=\"evolvingfungus/capit-v2-clip-baseline-True-2e-05-1e-05\",\n",
    "    model_name=\"latest.pt\",\n",
    "    backbone_fine_tunable=True,\n",
    "    batch=dummy_inputs,\n",
    ")\n",
    "baseline_pp = Ranker(\n",
    "    model_type=CLIPWithPostProcessingImageTextModel,\n",
    "    model_name_or_path=\"openai/clip-vit-large-patch14\",\n",
    "    repo_path=\"evolvingfungus/capit-v2-clip-with-post-processing-baseline-False-2e-05-1e-05-True\",\n",
    "    model_name=\"latest.pt\",\n",
    "    backbone_fine_tunable=True,\n",
    "    batch=dummy_inputs,\n",
    ")\n",
    "cap = Ranker(\n",
    "    model_type=CAPCLIPImageTextModel,\n",
    "    model_name_or_path=\"openai/clip-vit-large-patch14\",\n",
    "    repo_path=\"evolvingfungus/capit-v2-v1.1-cap-False-2e-05-1e-05-True\",\n",
    "    model_name=\"latest.pt\",\n",
    "    backbone_fine_tunable=True,\n",
    "    batch=dummy_inputs,\n",
    ")\n",
    "\n",
    "model_dict = {\"clip\": baseline, \"clip-pp\": baseline_pp, \"cap\": cap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from distutils.command.upload import upload\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def build_demo(\n",
    "    model_dict, collection_num_images: int = 0, challenge_num_images: int = 5\n",
    "):\n",
    "    with gr.Blocks() as demo:\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=224):\n",
    "                prompt = gr.Textbox(label=\"prompt\", value=\"\")\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=224):\n",
    "                rank_status = gr.Button(value=\"rank\", label=\"rank\")\n",
    "        with gr.Row():\n",
    "            if collection_num_images > 0:\n",
    "                collection_images = []\n",
    "                with gr.Column(scale=collection_num_images, min_width=224):\n",
    "                    for i in range(collection_num_images):\n",
    "                        collection_images.append(\n",
    "                            gr.Image(\n",
    "                                shape=(224, 224), label=f\"collection-image-{i}\"\n",
    "                            )\n",
    "                        )\n",
    "            with gr.Column(scale=challenge_num_images, min_width=224):\n",
    "                challenge_images = []\n",
    "                for i in range(challenge_num_images):\n",
    "                    challenge_images.append(\n",
    "                        gr.Image(shape=(224, 224), label=f\"challenge-image-{i}\")\n",
    "                    )\n",
    "            ranked_images_dict = defaultdict(list)\n",
    "            for key, model in model_dict.items():\n",
    "                with gr.Column(scale=challenge_num_images, min_width=224):\n",
    "                    for i in range(challenge_num_images):\n",
    "                        ranked_images_dict[key].append(\n",
    "                            gr.Image(\n",
    "                                shape=(224, 224),\n",
    "                                label=f\"ranked-image-{key}-{i}\",\n",
    "                            )\n",
    "                        )\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2, min_width=224):\n",
    "                challenge_upload_button = gr.UploadButton(\n",
    "                    \"Browse to select a folder with images for challenge images\",\n",
    "                    file_types=[\"image\"],\n",
    "                    file_count=\"multiple\",\n",
    "                )\n",
    "                challenge_upload_button.upload(\n",
    "                    upload_file, challenge_upload_button, challenge_images\n",
    "                )\n",
    "                collection_upload_button = gr.UploadButton(\n",
    "                    \"Browse to select a folder with images for collection images\",\n",
    "                    file_types=[\"image\"],\n",
    "                    file_count=\"multiple\",\n",
    "                )\n",
    "                collection_upload_button.upload(\n",
    "                    upload_file, collection_upload_button, collection_images\n",
    "                )\n",
    "                for model_name, model in model_dict.items():\n",
    "                    rank_status.click(\n",
    "                        fn=model.rank,\n",
    "                        inputs=[\n",
    "                            prompt,\n",
    "                            challenge_upload_button,\n",
    "                            collection_upload_button,\n",
    "                        ],\n",
    "                        outputs=ranked_images_dict[model_name],\n",
    "                    )\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_challenge_images = 50\n",
    "num_collection_images = 15\n",
    "demo = build_demo(\n",
    "    collection_num_images=num_collection_images,\n",
    "    challenge_num_images=num_challenge_images,\n",
    "    model_dict=model_dict,\n",
    ")\n",
    "demo.queue()\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8be86470e38501aa34785446b144512d347203ba2fe09ff4115a3f561b2ac78b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
